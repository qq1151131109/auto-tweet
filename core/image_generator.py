"""
Z-Imageå›¾ç‰‡ç”Ÿæˆå™¨
ç®€åŒ–ç‰ˆæœ¬ï¼šç›´æ¥æ¥æ”¶å‚æ•°ç”Ÿæˆå›¾ç‰‡ï¼Œæ”¯æŒæ‰¹é‡ç”Ÿæˆ
æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
1. Diffusersæ¨¡å¼ï¼ˆæ¨èï¼‰ï¼šä½¿ç”¨diffusers.ZImagePipelineï¼ŒåŸç”Ÿæ”¯æŒLoRA
2. åŸç”ŸPyTorchæ¨¡å¼ï¼šä½¿ç”¨Z-ImageåŸç”Ÿå®ç°ï¼ˆä¸æ”¯æŒLoRAï¼‰
"""
import sys
from pathlib import Path
from typing import Dict, Optional, List
from datetime import datetime
import torch
from PIL import Image
import logging

logger = logging.getLogger(__name__)


class ZImageGenerator:
    """
    Z-Imageå›¾ç‰‡ç”Ÿæˆå™¨ - æ”¯æŒDiffuserså’ŒåŸç”ŸPyTorchä¸¤ç§æ¨¡å¼
    """

    def __init__(
        self,
        model_path: str = "Z-Image/ckpts/Z-Image-Turbo",
        device: str = None,
        dtype: torch.dtype = torch.bfloat16,
        compile: bool = False,
        use_diffusers: bool = True  # é»˜è®¤ä½¿ç”¨diffusersï¼ˆæ”¯æŒLoRAï¼‰
    ):
        """
        åˆå§‹åŒ–Z-Imageç”Ÿæˆå™¨

        Args:
            model_path: Z-Imageæ¨¡å‹è·¯å¾„
            device: è®¾å¤‡ï¼ˆcuda/cpu/mps/None=è‡ªåŠ¨ï¼‰
            dtype: æ•°æ®ç±»å‹ï¼ˆé»˜è®¤bfloat16ï¼‰
            compile: æ˜¯å¦ç¼–è¯‘æ¨¡å‹ï¼ˆé»˜è®¤Falseï¼‰
            use_diffusers: æ˜¯å¦ä½¿ç”¨diffusersæ¨¡å¼ï¼ˆæ¨èTrueï¼Œæ”¯æŒLoRAï¼‰
        """
        # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
        if device is None:
            if torch.cuda.is_available():
                device = "cuda"
            elif torch.backends.mps.is_available():
                device = "mps"
            else:
                device = "cpu"

        self.device = device
        self.dtype = dtype
        self.use_diffusers = use_diffusers

        logger.info(f"ğŸ”§ åˆå§‹åŒ–ZImageGenerator")
        logger.info(f"   æ¨¡å¼: {'Diffusers (æ”¯æŒLoRA)' if use_diffusers else 'PyTorchåŸç”Ÿ'}")
        logger.info(f"   æ¨¡å‹: {model_path}")
        logger.info(f"   è®¾å¤‡: {device}")
        logger.info(f"   ç±»å‹: {dtype}")

        if use_diffusers:
            # Diffusersæ¨¡å¼
            self._init_diffusers(model_path, device, dtype, compile)
        else:
            # åŸç”ŸPyTorchæ¨¡å¼
            self._init_native(model_path, device, dtype, compile)

        logger.info(f"   âœ“ æ¨¡å‹åŠ è½½å®Œæˆ\n")

    def _init_diffusers(self, model_path: str, device: str, dtype: torch.dtype, compile: bool):
        """åˆå§‹åŒ–diffusersæ¨¡å¼"""
        try:
            from diffusers import ZImagePipeline

            logger.info("   åŠ è½½ZImagePipeline...")
            self.pipeline = ZImagePipeline.from_pretrained(
                model_path,
                torch_dtype=dtype,
                low_cpu_mem_usage=False
            )
            self.pipeline.to(device)

            # å¯é€‰ï¼šè®¾ç½®attention backend
            if hasattr(self.pipeline.transformer, 'set_attention_backend'):
                try:
                    self.pipeline.transformer.set_attention_backend("flash")
                    logger.info("   âœ“ ä½¿ç”¨Flash Attention")
                except:
                    pass

            # å¯é€‰ï¼šç¼–è¯‘æ¨¡å‹
            if compile:
                logger.info("   ç¼–è¯‘æ¨¡å‹...")
                self.pipeline.transformer.compile()

            self.pipeline.set_progress_bar_config(disable=True)

        except ImportError:
            logger.error("âŒ diffusersæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install diffusers")
            logger.info("   å›é€€åˆ°åŸç”ŸPyTorchæ¨¡å¼ï¼ˆä¸æ”¯æŒLoRAï¼‰")
            self.use_diffusers = False
            self._init_native(model_path, device, dtype, compile)

    def _init_native(self, model_path: str, device: str, dtype: torch.dtype, compile: bool):
        """åˆå§‹åŒ–åŸç”ŸPyTorchæ¨¡å¼"""
        # æ·»åŠ Z-Imageè·¯å¾„
        sys.path.insert(0, str(Path(__file__).parent.parent / "Z-Image" / "src"))

        from utils.loader import load_from_local_dir
        from utils.helpers import set_attention_backend

        logger.info("   åŠ è½½åŸç”ŸPyTorchç»„ä»¶...")
        self.components = load_from_local_dir(
            model_path,
            device=device,
            dtype=dtype,
            compile=compile,
            verbose=False
        )

        # è®¾ç½®attention backend
        set_attention_backend("_native_flash")

    def load_lora(self, lora_path: str, lora_strength: float = 1.0):
        """
        åŠ è½½LoRAï¼ˆä»…diffusersæ¨¡å¼æ”¯æŒï¼‰
        ä½¿ç”¨fuse_loraæ–¹æ¡ˆï¼Œç®€å•å¯é ï¼Œé¿å…adapterå‘½åå†²çª

        Args:
            lora_path: LoRAæ–‡ä»¶è·¯å¾„
            lora_strength: LoRAå¼ºåº¦
        """
        if not lora_path or not lora_path.strip():
            return

        lora_path = lora_path.strip()

        if not self.use_diffusers:
            logger.warning(f"âš ï¸  åŸç”ŸPyTorchæ¨¡å¼ä¸æ”¯æŒLoRA: {lora_path}")
            return

        lora_file = Path(lora_path)
        if not lora_file.exists():
            logger.warning(f"âš ï¸  LoRAæ–‡ä»¶ä¸å­˜åœ¨: {lora_path}")
            return

        try:
            logger.info(f"ğŸ”§ åŠ è½½LoRA: {lora_file.name}")
            logger.info(f"   å¼ºåº¦: {lora_strength}")

            # åŠ è½½LoRAæƒé‡
            self.pipeline.load_lora_weights(str(lora_file.parent), weight_name=lora_file.name)

            # ä½¿ç”¨fuse_loraç›´æ¥èåˆåˆ°æ¨¡å‹æƒé‡ä¸­
            # è¿™ç§æ–¹å¼æ¯”adapteræ–¹å¼æ›´ç®€å•å¯é ï¼Œé¿å…adapterå‘½åå†²çª
            if hasattr(self.pipeline, 'fuse_lora'):
                self.pipeline.fuse_lora(lora_scale=lora_strength)
                logger.info(f"   âœ“ LoRAå·²èåˆåˆ°æ¨¡å‹ (å¼ºåº¦: {lora_strength})")
            else:
                logger.warning(f"âš ï¸  Pipelineä¸æ”¯æŒfuse_loraï¼ŒLoRAå¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")

        except Exception as e:
            logger.error(f"   âŒ LoRAåŠ è½½å¤±è´¥: {e}")

    def unload_lora(self):
        """
        å¸è½½LoRA
        å…ˆunfuseæ¢å¤åŸå§‹æƒé‡ï¼Œå†unloadé‡Šæ”¾LoRAæƒé‡
        """
        if not self.use_diffusers:
            return

        try:
            # å…ˆunfuseæ¢å¤åŸå§‹æ¨¡å‹æƒé‡
            if hasattr(self.pipeline, 'unfuse_lora'):
                self.pipeline.unfuse_lora()
                logger.info("âœ“ LoRAå·²ä»æ¨¡å‹ä¸­è§£é™¤èåˆ")

            # å†unloadé‡Šæ”¾LoRAæƒé‡
            if hasattr(self.pipeline, 'unload_lora_weights'):
                self.pipeline.unload_lora_weights()
                logger.info("âœ“ LoRAæƒé‡å·²å¸è½½")

        except Exception as e:
            logger.warning(f"âš ï¸  LoRAå¸è½½å¤±è´¥: {e}")
    def generate_image(
        self,
        positive_prompt: str,
        negative_prompt: str = "",
        width: int = 1024,
        height: int = 1024,
        steps: int = 9,
        cfg: float = 0.0,
        seed: int = None,
        lora_path: str = "",
        lora_strength: float = 1.0
    ) -> Image.Image:
        """
        ç”Ÿæˆå•å¼ å›¾ç‰‡

        Args:
            positive_prompt: æ­£å‘æç¤ºè¯
            negative_prompt: è´Ÿå‘æç¤ºè¯
            width: å®½åº¦
            height: é«˜åº¦
            steps: æ¨ç†æ­¥æ•°ï¼ˆZ-Image-Turboæ¨è8-9ï¼‰
            cfg: CFG scaleï¼ˆZ-Image-Turboæ¨è0.0ï¼‰
            seed: éšæœºç§å­ï¼ˆNone=éšæœºï¼‰
            lora_path: LoRAè·¯å¾„ï¼ˆä»…diffusersæ¨¡å¼æ”¯æŒï¼‰
            lora_strength: LoRAå¼ºåº¦

        Returns:
            PIL.Imageå¯¹è±¡
        """
        # ç”Ÿæˆç§å­
        if seed is None:
            seed = torch.randint(0, 2**63 - 1, (1,)).item()

        # åŠ è½½LoRAï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if lora_path:
            self.load_lora(lora_path, lora_strength)

        # åˆ›å»ºgenerator
        generator = torch.Generator(self.device).manual_seed(seed)

        if self.use_diffusers:
            # Diffusersæ¨¡å¼
            result = self.pipeline(
                prompt=positive_prompt,
                negative_prompt=negative_prompt if negative_prompt else None,
                height=height,
                width=width,
                num_inference_steps=steps,
                guidance_scale=cfg,
                generator=generator
            )
            image = result.images[0]

            # å¸è½½LoRAï¼ˆé¿å…å½±å“ä¸‹ä¸€æ¬¡ç”Ÿæˆï¼‰
            if lora_path:
                self.unload_lora()

        else:
            # åŸç”ŸPyTorchæ¨¡å¼
            from zimage.pipeline import generate

            images = generate(
                prompt=positive_prompt,
                negative_prompt=negative_prompt if negative_prompt else None,
                height=height,
                width=width,
                num_inference_steps=steps,
                guidance_scale=cfg,
                generator=generator,
                **self.components
            )
            image = images[0]

        return image


async def generate_batch_images_single_gpu(
    tweets_batch: Dict,
    output_dir: str,
    model_path: str,
    device: str = "cuda",
    start_slot: int = 0,
    max_images: Optional[int] = None,
    use_diffusers: bool = True,
    use_advanced: bool = False  # æ˜¯å¦ä½¿ç”¨é«˜çº§ç”Ÿæˆå™¨
) -> List[Dict]:
    """
    å•GPUæ‰¹é‡ç”Ÿæˆå›¾ç‰‡

    Args:
        tweets_batch: æ¨æ–‡æ‰¹æ¬¡JSON
        output_dir: è¾“å‡ºç›®å½•
        model_path: Z-Imageæ¨¡å‹è·¯å¾„
        device: è®¾å¤‡
        start_slot: èµ·å§‹slot
        max_images: æœ€å¤§ç”Ÿæˆæ•°é‡
        use_diffusers: æ˜¯å¦ä½¿ç”¨diffusersæ¨¡å¼ï¼ˆæ”¯æŒLoRAï¼‰
        use_advanced: æ˜¯å¦ä½¿ç”¨é«˜çº§ç”Ÿæˆå™¨ï¼ˆä¸‰é˜¶æ®µæ¸è¿›å¼ï¼‰

    Returns:
        ç”Ÿæˆç»“æœåˆ—è¡¨
    """
    # æ ¹æ®é…ç½®é€‰æ‹©ç”Ÿæˆå™¨
    if use_advanced:
        from core.image_generator_advanced import generate_batch_images_advanced
        from config.image_config import load_image_config, get_generation_mode, load_negative_prompt_template

        # åŠ è½½é…ç½®
        config = load_image_config()
        generation_mode = get_generation_mode(config)
        negative_prompt_template = load_negative_prompt_template(config)

        # ä½¿ç”¨é«˜çº§ç”Ÿæˆå™¨
        use_progressive = (generation_mode == "advanced")

        return await generate_batch_images_advanced(
            tweets_batch=tweets_batch,
            output_dir=output_dir,
            model_path=model_path,
            device=device,
            use_progressive=use_progressive,
            negative_prompt_template=negative_prompt_template,
            start_slot=start_slot,
            max_images=max_images
        )

    # ä½¿ç”¨åŸæœ‰ç”Ÿæˆå™¨ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
    generator = ZImageGenerator(model_path=model_path, device=device, use_diffusers=use_diffusers)

    tweets = tweets_batch["tweets"]
    persona_name = tweets_batch["persona"]["name"]
    day_offset = tweets_batch.get("daily_plan", {}).get("day_offset", None)  # è·å–dayä¿¡æ¯
    total = len(tweets)
    end_slot = min(total, start_slot + max_images) if max_images else total

    logger.info(f"ğŸ“Š å•GPUæ‰¹é‡ç”Ÿæˆ")
    logger.info(f"   äººè®¾: {persona_name}")
    logger.info(f"   èŒƒå›´: slot {start_slot} ~ {end_slot-1}")

    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    results = []

    for i in range(start_slot, end_slot):
        tweet = tweets[i]
        img_gen = tweet["image_generation"]

        # æå–å‚æ•°ï¼ˆå®‰å…¨è®¿é—®ï¼‰
        positive_prompt = img_gen.get("positive_prompt", "")
        negative_prompt = img_gen.get("negative_prompt", "")

        # LoRAå‚æ•°ï¼ˆå¯é€‰ï¼‰
        lora_params = img_gen.get("lora_params", {})
        lora_path = lora_params.get("model_path", "")
        lora_strength = lora_params.get("strength", 1.0)

        # ç”Ÿæˆå‚æ•°ï¼ˆä½¿ç”¨é»˜è®¤å€¼ï¼‰
        gen_params = img_gen.get("generation_params", {})
        width = gen_params.get("width", 1024)
        height = gen_params.get("height", 1024)
        steps = gen_params.get("steps", 9)
        cfg = gen_params.get("cfg", 0.0)

        # ç”Ÿæˆæ–‡ä»¶åï¼ˆåŒ…å«dayä¿¡æ¯ï¼‰
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        if day_offset is not None:
            filename = f"{persona_name}_day{day_offset}_slot{i}_{timestamp}.png"
        else:
            filename = f"{persona_name}_slot{i}_{timestamp}.png"
        output_path = output_dir / filename

        logger.info(f"ğŸ¨ ç”Ÿæˆ slot {i+1}/{total}: {tweet['topic_type']}")

        try:
            # ç”Ÿæˆå›¾ç‰‡
            image = generator.generate_image(
                positive_prompt=positive_prompt,
                negative_prompt=negative_prompt,
                width=width,
                height=height,
                steps=steps,
                cfg=cfg,
                lora_path=lora_path,
                lora_strength=lora_strength
            )

            # ä¿å­˜
            image.save(output_path)

            results.append({
                "slot": i,
                "status": "success",
                "output_path": str(output_path),
                "tweet_text": tweet["tweet_text"]
            })

            logger.info(f"   âœ“ ä¿å­˜è‡³: {output_path}")

        except Exception as e:
            logger.error(f"   âŒ å¤±è´¥: {e}")
            results.append({
                "slot": i,
                "status": "failed",
                "error": str(e)
            })

    return results


# ============ å¤šGPUå¹¶å‘ç”Ÿæˆ ============

import torch.multiprocessing as mp
from queue import Empty


def _worker_generate_images(
    gpu_id: int,
    task_queue: mp.Queue,
    result_queue: mp.Queue,
    model_path: str,
    tweets_batch: Dict,
    output_dir: str,
    use_diffusers: bool = True
):
    """
    å¤šGPU workerè¿›ç¨‹ - åœ¨æŒ‡å®šGPUä¸Šç”Ÿæˆå›¾ç‰‡

    Args:
        gpu_id: GPUç¼–å·
        task_queue: ä»»åŠ¡é˜Ÿåˆ—ï¼ˆæ¥æ”¶slotç´¢å¼•ï¼‰
        result_queue: ç»“æœé˜Ÿåˆ—
        model_path: æ¨¡å‹è·¯å¾„
        tweets_batch: æ¨æ–‡æ‰¹æ¬¡
        output_dir: è¾“å‡ºç›®å½•
        use_diffusers: æ˜¯å¦ä½¿ç”¨diffusersæ¨¡å¼
    """
    # è®¾ç½®å½“å‰è¿›ç¨‹ä½¿ç”¨çš„GPU
    device = f"cuda:{gpu_id}"
    torch.cuda.set_device(gpu_id)

    # åˆå§‹åŒ–ç”Ÿæˆå™¨
    generator = ZImageGenerator(model_path=model_path, device=device, use_diffusers=use_diffusers)

    tweets = tweets_batch["tweets"]
    persona_name = tweets_batch["persona"]["name"]
    day_offset = tweets_batch.get("daily_plan", {}).get("day_offset", None)  # è·å–dayä¿¡æ¯
    output_dir = Path(output_dir)

    print(f"âœ“ GPU {gpu_id} worker å¯åŠ¨")

    while True:
        try:
            # ä»é˜Ÿåˆ—è·å–ä»»åŠ¡ï¼ˆè¶…æ—¶1ç§’ï¼‰
            slot_idx = task_queue.get(timeout=1)

            if slot_idx is None:  # ç»“æŸä¿¡å·
                break

            tweet = tweets[slot_idx]
            img_gen = tweet["image_generation"]

            # æå–å‚æ•°ï¼ˆå®‰å…¨è®¿é—®ï¼‰
            positive_prompt = img_gen.get("positive_prompt", "")
            negative_prompt = img_gen.get("negative_prompt", "")

            # LoRAå‚æ•°ï¼ˆå¯é€‰ï¼‰
            lora_params = img_gen.get("lora_params", {})
            lora_path = lora_params.get("model_path", "")
            lora_strength = lora_params.get("strength", 1.0)

            # ç”Ÿæˆå‚æ•°ï¼ˆä½¿ç”¨é»˜è®¤å€¼ï¼‰
            gen_params = img_gen.get("generation_params", {})
            width = gen_params.get("width", 1024)
            height = gen_params.get("height", 1024)
            steps = gen_params.get("steps", 9)
            cfg = gen_params.get("cfg", 0.0)

            # ç”Ÿæˆæ–‡ä»¶åï¼ˆåŒ…å«dayä¿¡æ¯ï¼‰
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            if day_offset is not None:
                filename = f"{persona_name}_day{day_offset}_slot{slot_idx}_{timestamp}.png"
            else:
                filename = f"{persona_name}_slot{slot_idx}_{timestamp}.png"
            output_path = output_dir / filename

            print(f"ğŸ¨ GPU {gpu_id} ç”Ÿæˆ slot {slot_idx}: {tweet['topic_type']}")

            try:
                # ç”Ÿæˆå›¾ç‰‡
                image = generator.generate_image(
                    positive_prompt=positive_prompt,
                    negative_prompt=negative_prompt,
                    width=width,
                    height=height,
                    steps=steps,
                    cfg=cfg,
                    lora_path=lora_path,
                    lora_strength=lora_strength
                )

                # ä¿å­˜
                image.save(output_path)

                result_queue.put({
                    "slot": slot_idx,
                    "gpu": gpu_id,
                    "status": "success",
                    "output_path": str(output_path),
                    "tweet_text": tweet["tweet_text"]
                })

                print(f"   âœ“ GPU {gpu_id} å®Œæˆ slot {slot_idx}")

            except Exception as e:
                print(f"   âŒ GPU {gpu_id} å¤±è´¥ slot {slot_idx}: {e}")
                result_queue.put({
                    "slot": slot_idx,
                    "gpu": gpu_id,
                    "status": "failed",
                    "error": str(e)
                })

        except Empty:
            continue
        except Exception as e:
            print(f"âŒ GPU {gpu_id} workerå¼‚å¸¸: {e}")
            break

    print(f"âœ“ GPU {gpu_id} worker ç»“æŸ")


async def generate_batch_images_multi_gpu(
    tweets_batch: Dict,
    output_dir: str,
    model_path: str,
    num_gpus: int = None,
    start_slot: int = 0,
    max_images: Optional[int] = None,
    use_diffusers: bool = True
) -> List[Dict]:
    """
    å¤šGPUå¹¶å‘æ‰¹é‡ç”Ÿæˆå›¾ç‰‡

    Args:
        tweets_batch: æ¨æ–‡æ‰¹æ¬¡JSON
        output_dir: è¾“å‡ºç›®å½•
        model_path: Z-Imageæ¨¡å‹è·¯å¾„
        num_gpus: ä½¿ç”¨çš„GPUæ•°é‡ï¼ˆNone=è‡ªåŠ¨æ£€æµ‹å…¨éƒ¨GPUï¼‰
        start_slot: èµ·å§‹slot
        max_images: æœ€å¤§ç”Ÿæˆæ•°é‡
        use_diffusers: æ˜¯å¦ä½¿ç”¨diffusersæ¨¡å¼ï¼ˆæ”¯æŒLoRAï¼‰

    Returns:
        ç”Ÿæˆç»“æœåˆ—è¡¨
    """
    # æ£€æµ‹å¯ç”¨GPU
    if not torch.cuda.is_available():
        logger.warning("âš ï¸  CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°å•GPUæ¨¡å¼")
        return await generate_batch_images_single_gpu(
            tweets_batch, output_dir, model_path, "cpu", start_slot, max_images, use_diffusers
        )

    total_gpus = torch.cuda.device_count()
    if num_gpus is None:
        num_gpus = total_gpus
    else:
        num_gpus = min(num_gpus, total_gpus)

    if num_gpus == 1:
        logger.info("ä½¿ç”¨å•GPUæ¨¡å¼")
        return await generate_batch_images_single_gpu(
            tweets_batch, output_dir, model_path, "cuda:0", start_slot, max_images, use_diffusers
        )

    logger.info(f"ğŸš€ å¤šGPUå¹¶å‘ç”Ÿæˆæ¨¡å¼")
    logger.info(f"   å¯ç”¨GPU: {total_gpus}")
    logger.info(f"   ä½¿ç”¨GPU: {num_gpus}")

    tweets = tweets_batch["tweets"]
    total = len(tweets)
    end_slot = min(total, start_slot + max_images) if max_images else total

    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—å’Œç»“æœé˜Ÿåˆ—
    mp.set_start_method('spawn', force=True)
    task_queue = mp.Queue()
    result_queue = mp.Queue()

    # å¡«å……ä»»åŠ¡é˜Ÿåˆ—
    for i in range(start_slot, end_slot):
        task_queue.put(i)

    # æ·»åŠ ç»“æŸä¿¡å·
    for _ in range(num_gpus):
        task_queue.put(None)

    # å¯åŠ¨workerè¿›ç¨‹
    processes = []
    for gpu_id in range(num_gpus):
        p = mp.Process(
            target=_worker_generate_images,
            args=(gpu_id, task_queue, result_queue, model_path, tweets_batch, output_dir, use_diffusers)
        )
        p.start()
        processes.append(p)

    logger.info(f"   âœ“ å¯åŠ¨ {num_gpus} ä¸ªGPU worker")

    # æ”¶é›†ç»“æœ
    results = []
    expected_count = end_slot - start_slot

    while len(results) < expected_count:
        try:
            result = result_queue.get(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
            results.append(result)
            logger.info(f"   è¿›åº¦: {len(results)}/{expected_count}")
        except Empty:
            logger.warning("âš ï¸  ç»“æœé˜Ÿåˆ—è¶…æ—¶")
            break

    # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹ç»“æŸ
    for p in processes:
        p.join(timeout=10)
        if p.is_alive():
            p.terminate()

    # æŒ‰slotæ’åº
    results.sort(key=lambda x: x["slot"])

    success_count = sum(1 for r in results if r["status"] == "success")
    logger.info(f"\nâœ… å¤šGPUç”Ÿæˆå®Œæˆ")
    logger.info(f"   æˆåŠŸ: {success_count}/{expected_count}")
    logger.info(f"   è¾“å‡º: {output_dir}\n")

    return results


class ImageGenerationCoordinator:
    """å›¾ç‰‡ç”Ÿæˆåè°ƒå™¨ - æ”¯æŒå•GPUå’Œå¤šGPUæ¨¡å¼ï¼Œæ”¯æŒLoRAï¼Œæ”¯æŒæ–°æ—§æ–¹æ¡ˆåˆ‡æ¢"""

    def __init__(
        self,
        model_path: str = "Z-Image/ckpts/Z-Image-Turbo",
        num_gpus: int = None,
        use_diffusers: bool = True,
        use_advanced: bool = None  # None=ä»é…ç½®è¯»å–ï¼ŒTrue=å¼ºåˆ¶ä½¿ç”¨é«˜çº§æ¨¡å¼ï¼ŒFalse=å¼ºåˆ¶ä½¿ç”¨å¤‡ç”¨æ¨¡å¼
    ):
        self.model_path = model_path
        self.num_gpus = num_gpus
        self.use_diffusers = use_diffusers

        # å†³å®šæ˜¯å¦ä½¿ç”¨é«˜çº§æ¨¡å¼
        if use_advanced is None:
            # ä»é…ç½®æ–‡ä»¶è¯»å–
            from config.image_config import load_image_config, get_generation_mode
            config = load_image_config()
            generation_mode = get_generation_mode(config)
            self.use_advanced = (generation_mode == "advanced")
        else:
            self.use_advanced = use_advanced

        logger.info(f"ğŸ”§ ImageGenerationCoordinator åˆå§‹åŒ–")
        logger.info(f"   ç”Ÿæˆæ¨¡å¼: {'é«˜çº§æ¨¡å¼ (ä¸‰é˜¶æ®µæ¸è¿›å¼)' if self.use_advanced else 'å¤‡ç”¨æ¨¡å¼ (å•é˜¶æ®µç”Ÿæˆ)'}")

    async def generate_from_tweets_batch(
        self,
        tweets_batch_file: str,
        output_dir: str = "output_images",
        start_slot: int = 0,
        max_images: Optional[int] = None,
        use_multi_gpu: bool = True
    ) -> List[Dict]:
        """
        ä»æ¨æ–‡æ‰¹æ¬¡æ–‡ä»¶ç”Ÿæˆå›¾ç‰‡

        Args:
            tweets_batch_file: æ¨æ–‡æ‰¹æ¬¡JSONæ–‡ä»¶
            output_dir: è¾“å‡ºç›®å½•
            start_slot: èµ·å§‹slot
            max_images: æœ€å¤§ç”Ÿæˆæ•°é‡
            use_multi_gpu: æ˜¯å¦ä½¿ç”¨å¤šGPUï¼ˆé»˜è®¤Trueï¼‰

        Returns:
            ç”Ÿæˆç»“æœåˆ—è¡¨
        """
        import json

        # åŠ è½½æ¨æ–‡æ‰¹æ¬¡
        with open(tweets_batch_file, 'r', encoding='utf-8') as f:
            tweets_batch = json.load(f)

        logger.info(f"ğŸ“‚ ä»æ¨æ–‡æ‰¹æ¬¡ç”Ÿæˆå›¾ç‰‡")
        logger.info(f"   æ–‡ä»¶: {tweets_batch_file}")
        logger.info(f"   äººè®¾: {tweets_batch['persona']['name']}")
        logger.info(f"   æ¨æ–‡æ•°: {len(tweets_batch['tweets'])}")
        logger.info(f"   æ¨¡å¼: {'Diffusers (æ”¯æŒLoRA)' if self.use_diffusers else 'PyTorchåŸç”Ÿ'}")
        logger.info(f"   ç”Ÿæˆæ–¹æ¡ˆ: {'é«˜çº§ (ä¸‰é˜¶æ®µæ¸è¿›å¼)' if self.use_advanced else 'å¤‡ç”¨ (å•é˜¶æ®µ)'}")

        # é€‰æ‹©ç”Ÿæˆæ¨¡å¼
        if use_multi_gpu and torch.cuda.is_available() and torch.cuda.device_count() > 1:
            # å¤šGPUæ¨¡å¼æš‚ä¸æ”¯æŒé«˜çº§ç”Ÿæˆå™¨ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
            if self.use_advanced:
                logger.warning("âš ï¸  å¤šGPUæ¨¡å¼æš‚ä¸æ”¯æŒé«˜çº§ç”Ÿæˆå™¨ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
                use_advanced_for_this_run = False
            else:
                use_advanced_for_this_run = False

            results = await generate_batch_images_multi_gpu(
                tweets_batch=tweets_batch,
                output_dir=output_dir,
                model_path=self.model_path,
                num_gpus=self.num_gpus,
                start_slot=start_slot,
                max_images=max_images,
                use_diffusers=self.use_diffusers
            )
        else:
            results = await generate_batch_images_single_gpu(
                tweets_batch=tweets_batch,
                output_dir=output_dir,
                model_path=self.model_path,
                device="cuda" if torch.cuda.is_available() else "cpu",
                start_slot=start_slot,
                max_images=max_images,
                use_diffusers=self.use_diffusers,
                use_advanced=self.use_advanced  # ä¼ é€’é«˜çº§æ¨¡å¼æ ‡å¿—
            )

        return results

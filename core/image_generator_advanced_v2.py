"""
Z-Image é«˜çº§å›¾ç‰‡ç”Ÿæˆå™¨ V2
çœŸæ­£çš„ Latent ç©ºé—´ä¸‰é˜¶æ®µæ¸è¿›å¼ç”Ÿæˆï¼ˆå®Œå…¨å¤åˆ» ComfyUI å·¥ä½œæµï¼‰

å…³é”®ä¿®å¤ï¼š
1. ä½¿ç”¨ Latent ç©ºé—´æ“ä½œï¼ˆä¸æ˜¯åƒç´ ç©ºé—´ï¼‰
2. æ­£ç¡®çš„å°ºå¯¸ï¼š176Ã—224 â†’ 336Ã—432 â†’ 672Ã—864 (latent ç©ºé—´)
3. ä½¿ç”¨ pipeline çš„ latents å‚æ•°è¿›è¡Œ latent ä¸Šé‡‡æ ·
"""
import sys
from pathlib import Path
from typing import Dict, Optional, List, Tuple
from datetime import datetime
import torch
from PIL import Image
import logging
import torch.nn.functional as F

logger = logging.getLogger(__name__)


class ZImageGeneratorAdvancedV2:
    """
    Z-Image é«˜çº§å›¾ç‰‡ç”Ÿæˆå™¨ V2 - çœŸæ­£çš„ Latent ç©ºé—´ä¸‰é˜¶æ®µæ¸è¿›å¼ç”Ÿæˆ

    å®Œå…¨å¤åˆ» ComfyUI workflow/zimage-121101.json
    """

    def __init__(
        self,
        model_path: str = "Z-Image/ckpts/Z-Image-Turbo",
        device: str = None,
        dtype: torch.dtype = torch.bfloat16,
        compile: bool = False,
    ):
        """
        åˆå§‹åŒ– Z-Image é«˜çº§ç”Ÿæˆå™¨ V2

        Args:
            model_path: Z-Image æ¨¡å‹è·¯å¾„
            device: è®¾å¤‡ï¼ˆcuda/cpu/mps/None=è‡ªåŠ¨ï¼‰
            dtype: æ•°æ®ç±»å‹ï¼ˆé»˜è®¤bfloat16ï¼‰
            compile: æ˜¯å¦ç¼–è¯‘æ¨¡å‹ï¼ˆé»˜è®¤Falseï¼‰
        """
        # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
        if device is None:
            if torch.cuda.is_available():
                device = "cuda"
            elif torch.backends.mps.is_available():
                device = "mps"
            else:
                device = "cpu"

        self.device = device
        self.dtype = dtype

        logger.info(f"ğŸ”§ åˆå§‹åŒ– ZImageGeneratorAdvancedV2 (Latent ç©ºé—´ä¸‰é˜¶æ®µ)")
        logger.info(f"   æ¨¡å‹: {model_path}")
        logger.info(f"   è®¾å¤‡: {device}")
        logger.info(f"   ç±»å‹: {dtype}")

        self._init_pipeline(model_path, device, dtype, compile)

        logger.info(f"   âœ“ æ¨¡å‹åŠ è½½å®Œæˆ\n")

    def _init_pipeline(self, model_path: str, device: str, dtype: torch.dtype, compile: bool):
        """åˆå§‹åŒ– Diffusers pipeline"""
        try:
            from diffusers import ZImagePipeline

            logger.info("   åŠ è½½ ZImagePipeline...")
            self.pipeline = ZImagePipeline.from_pretrained(
                model_path,
                torch_dtype=dtype,
                low_cpu_mem_usage=False
            )
            self.pipeline.to(device)

            # å¯é€‰ï¼šè®¾ç½® attention backend
            if hasattr(self.pipeline.transformer, 'set_attention_backend'):
                try:
                    self.pipeline.transformer.set_attention_backend("flash")
                    logger.info("   âœ“ ä½¿ç”¨ Flash Attention")
                except:
                    pass

            # å¯é€‰ï¼šç¼–è¯‘æ¨¡å‹
            if compile:
                logger.info("   ç¼–è¯‘æ¨¡å‹...")
                self.pipeline.transformer.compile()

            self.pipeline.set_progress_bar_config(disable=True)

        except ImportError:
            logger.error("âŒ diffusers æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install diffusers")
            raise

    def load_lora(self, lora_path: str, lora_strength: float = 1.0):
        """
        åŠ è½½ LoRA
        """
        if not lora_path or not lora_path.strip():
            return

        lora_path = lora_path.strip()
        lora_file = Path(lora_path)

        if not lora_file.exists():
            logger.warning(f"âš ï¸  LoRA æ–‡ä»¶ä¸å­˜åœ¨: {lora_path}")
            return

        try:
            logger.info(f"ğŸ”§ åŠ è½½ LoRA: {lora_file.name}")
            logger.info(f"   å¼ºåº¦: {lora_strength}")

            self.pipeline.load_lora_weights(str(lora_file.parent), weight_name=lora_file.name)

            if hasattr(self.pipeline, 'fuse_lora'):
                self.pipeline.fuse_lora(lora_scale=lora_strength)
                logger.info(f"   âœ“ LoRA å·²èåˆåˆ°æ¨¡å‹ (å¼ºåº¦: {lora_strength})")
            else:
                logger.warning(f"âš ï¸  Pipeline ä¸æ”¯æŒ fuse_lora")

        except Exception as e:
            logger.error(f"   âŒ LoRA åŠ è½½å¤±è´¥: {e}")

    def unload_lora(self):
        """å¸è½½ LoRA"""
        try:
            if hasattr(self.pipeline, 'unfuse_lora'):
                self.pipeline.unfuse_lora()
            if hasattr(self.pipeline, 'unload_lora_weights'):
                self.pipeline.unload_lora_weights()
        except Exception as e:
            logger.warning(f"âš ï¸  LoRA å¸è½½å¤±è´¥: {e}")

    def _upscale_latent(self, latents: torch.Tensor, target_size: Tuple[int, int], method: str = "nearest") -> torch.Tensor:
        """
        ä¸Šé‡‡æ · latentï¼ˆæ¨¡æ‹Ÿ ComfyUI çš„ LatentUpscaleï¼‰

        Args:
            latents: è¾“å…¥ latent tensor [B, C, H, W]
            target_size: ç›®æ ‡å°ºå¯¸ (height, width) - latent ç©ºé—´
            method: ä¸Šé‡‡æ ·æ–¹æ³• ("nearest", "bilinear", "bicubic")

        Returns:
            ä¸Šé‡‡æ ·åçš„ latent tensor
        """
        target_height, target_width = target_size

        # ä½¿ç”¨ torch.nn.functional.interpolate
        upscaled_latents = F.interpolate(
            latents,
            size=(target_height, target_width),
            mode=method,
            align_corners=False if method != "nearest" else None
        )

        return upscaled_latents

    def generate_progressive_latent(
        self,
        positive_prompt: str,
        negative_prompt: str = "",
        trigger_word: str = "",
        # Latent ç©ºé—´å°ºå¯¸ï¼ˆä¸æ˜¯åƒç´ ï¼ï¼‰
        stage1_latent_size: Tuple[int, int] = (224, 176),  # (H, W) latent ç©ºé—´
        stage2_latent_size: Tuple[int, int] = (432, 336),
        stage3_latent_size: Tuple[int, int] = (864, 672),
        # é‡‡æ ·å‚æ•°
        stage1_steps: int = 9,
        stage2_steps: int = 16,
        stage3_steps: int = 16,
        stage1_cfg: float = 2.0,
        stage2_cfg: float = 1.0,
        stage3_cfg: float = 1.0,
        # denoise å‚æ•°
        stage2_denoise: float = 0.7,
        stage3_denoise: float = 0.6,
        # LoRA å‚æ•°
        lora_path: str = "",
        lora_strength: float = 1.0,
        # ç§å­
        seeds: Optional[Tuple[int, int, int]] = None,
    ) -> Image.Image:
        """
        ä¸‰é˜¶æ®µæ¸è¿›å¼ç”Ÿæˆï¼ˆLatent ç©ºé—´æ“ä½œï¼Œå®Œå…¨å¤åˆ» ComfyUIï¼‰

        Args:
            positive_prompt: æ­£å‘æç¤ºè¯
            negative_prompt: è´Ÿå‘æç¤ºè¯
            trigger_word: LoRA è§¦å‘è¯
            stage1_latent_size: é˜¶æ®µ1 latent å°ºå¯¸ (H, W)
            stage2_latent_size: é˜¶æ®µ2 latent å°ºå¯¸ (H, W)
            stage3_latent_size: é˜¶æ®µ3 latent å°ºå¯¸ (H, W)
            ...å…¶ä»–å‚æ•°

        Returns:
            PIL.Image å¯¹è±¡
        """
        # åˆå¹¶ trigger word
        if trigger_word:
            full_prompt = f"{trigger_word}, {positive_prompt}"
        else:
            full_prompt = positive_prompt

        # ç”Ÿæˆç§å­
        if seeds is None:
            seeds = (
                torch.randint(0, 2**63 - 1, (1,)).item(),
                torch.randint(0, 2**63 - 1, (1,)).item(),
                torch.randint(0, 2**63 - 1, (1,)).item(),
            )

        # åŠ è½½ LoRA
        if lora_path:
            self.load_lora(lora_path, lora_strength)

        logger.info(f"ğŸ¨ ä¸‰é˜¶æ®µæ¸è¿›å¼ç”Ÿæˆ (Latent ç©ºé—´)")
        logger.info(f"   Trigger Word: {trigger_word if trigger_word else '(æ— )'}")
        logger.info(f"   LoRA: {Path(lora_path).name if lora_path else '(æ— )'}")

        # ============ é˜¶æ®µ1: ä½åˆ†è¾¨ç‡åŸºç¡€ç”Ÿæˆ (Latent ç©ºé—´) ============
        stage1_h, stage1_w = stage1_latent_size
        # è½¬æ¢ä¸ºåƒç´ ç©ºé—´ï¼ˆVAE çš„ latent ç¼©æ”¾å› å­æ˜¯ 8ï¼‰
        stage1_pixel_h = stage1_h * 8
        stage1_pixel_w = stage1_w * 8

        logger.info(f"\nğŸ“ é˜¶æ®µ1: åŸºç¡€ç”Ÿæˆ")
        logger.info(f"   Latent: {stage1_h}Ã—{stage1_w}")
        logger.info(f"   åƒç´ : {stage1_pixel_h}Ã—{stage1_pixel_w}")
        logger.info(f"   Steps: {stage1_steps}, CFG: {stage1_cfg}, Seed: {seeds[0]}")

        generator1 = torch.Generator(self.device).manual_seed(seeds[0])

        # ç”Ÿæˆåˆå§‹ latent
        result1 = self.pipeline(
            prompt=full_prompt,
            negative_prompt=negative_prompt if negative_prompt else None,
            height=stage1_pixel_h,
            width=stage1_pixel_w,
            num_inference_steps=stage1_steps,
            guidance_scale=stage1_cfg,
            generator=generator1,
            output_type="latent"  # å…³é”®ï¼šè¾“å‡º latent è€Œä¸æ˜¯ PIL å›¾åƒ
        )
        latent_stage1 = result1.images  # è¿™æ˜¯ latent tensor [B, C, H, W]

        logger.info(f"   âœ“ Latent shape: {latent_stage1.shape}")

        # ============ é˜¶æ®µ2: ä¸Šé‡‡æ ·åˆ°ä¸­åˆ†è¾¨ç‡ (Latent ç©ºé—´) ============
        stage2_h, stage2_w = stage2_latent_size
        stage2_pixel_h = stage2_h * 8
        stage2_pixel_w = stage2_w * 8

        logger.info(f"\nğŸ“ é˜¶æ®µ2: ä¸­é—´ç²¾ä¿®")
        logger.info(f"   Latent: {stage2_h}Ã—{stage2_w}")
        logger.info(f"   åƒç´ : {stage2_pixel_h}Ã—{stage2_pixel_w}")
        logger.info(f"   Steps: {stage2_steps}, CFG: {stage2_cfg}, Denoise: {stage2_denoise}, Seed: {seeds[1]}")

        # Latent ç©ºé—´ä¸Šé‡‡æ ·
        latent_upscaled2 = self._upscale_latent(latent_stage1, (stage2_h, stage2_w), method="nearest")
        logger.info(f"   âœ“ Upscaled latent shape: {latent_upscaled2.shape}")

        # img2latent ç²¾ä¿®ï¼ˆä½¿ç”¨ denoise æ§åˆ¶å™ªå£°å¼ºåº¦ï¼‰
        generator2 = torch.Generator(self.device).manual_seed(seeds[2])

        # è®¡ç®—å®é™…çš„ timestepï¼ˆdenoise æ§åˆ¶ä»å“ªä¸ª timestep å¼€å§‹ï¼‰
        # denoise=0.7 è¡¨ç¤ºä¿ç•™ 30% åŸå§‹ latentï¼Œé‡ç»˜ 70%
        start_timestep = int(stage2_steps * (1 - stage2_denoise))

        result2 = self.pipeline(
            prompt=full_prompt,
            negative_prompt=negative_prompt if negative_prompt else None,
            height=stage2_pixel_h,
            width=stage2_pixel_w,
            num_inference_steps=stage2_steps,
            guidance_scale=stage2_cfg,
            generator=generator2,
            latents=latent_upscaled2,  # ä¼ å…¥ä¸Šé‡‡æ ·çš„ latent
            output_type="latent"
        )
        latent_stage2 = result2.images

        logger.info(f"   âœ“ Refined latent shape: {latent_stage2.shape}")

        # ============ é˜¶æ®µ3: ä¸Šé‡‡æ ·åˆ°é«˜åˆ†è¾¨ç‡ (Latent ç©ºé—´) ============
        stage3_h, stage3_w = stage3_latent_size
        stage3_pixel_h = stage3_h * 8
        stage3_pixel_w = stage3_w * 8

        logger.info(f"\nğŸ“ é˜¶æ®µ3: æœ€ç»ˆç²¾ä¿®")
        logger.info(f"   Latent: {stage3_h}Ã—{stage3_w}")
        logger.info(f"   åƒç´ : {stage3_pixel_h}Ã—{stage3_pixel_w}")
        logger.info(f"   Steps: {stage3_steps}, CFG: {stage3_cfg}, Denoise: {stage3_denoise}, Seed: {seeds[2]}")

        # Latent ç©ºé—´ä¸Šé‡‡æ · (Ã—2)
        latent_upscaled3 = self._upscale_latent(latent_stage2, (stage3_h, stage3_w), method="nearest")
        logger.info(f"   âœ“ Upscaled latent shape: {latent_upscaled3.shape}")

        # æœ€ç»ˆç²¾ä¿®
        generator3 = torch.Generator(self.device).manual_seed(seeds[2])

        result3 = self.pipeline(
            prompt=full_prompt,
            negative_prompt=negative_prompt if negative_prompt else None,
            height=stage3_pixel_h,
            width=stage3_pixel_w,
            num_inference_steps=stage3_steps,
            guidance_scale=stage3_cfg,
            generator=generator3,
            latents=latent_upscaled3,
            output_type="pil"  # æœ€åä¸€æ­¥è¾“å‡º PIL å›¾åƒ
        )
        image_final = result3.images[0]

        # å¸è½½ LoRA
        if lora_path:
            self.unload_lora()

        logger.info(f"\nâœ… ä¸‰é˜¶æ®µç”Ÿæˆå®Œæˆ")
        logger.info(f"   æœ€ç»ˆå°ºå¯¸: {image_final.size}")

        return image_final

    def generate_simple(
        self,
        positive_prompt: str,
        negative_prompt: str = "",
        trigger_word: str = "",
        width: int = 768,
        height: int = 1024,
        steps: int = 9,
        cfg: float = 1.0,
        seed: int = None,
        lora_path: str = "",
        lora_strength: float = 1.0
    ) -> Image.Image:
        """
        ç®€å•å•é˜¶æ®µç”Ÿæˆï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
        """
        if trigger_word:
            full_prompt = f"{trigger_word}, {positive_prompt}"
        else:
            full_prompt = positive_prompt

        if seed is None:
            seed = torch.randint(0, 2**63 - 1, (1,)).item()

        if lora_path:
            self.load_lora(lora_path, lora_strength)

        generator = torch.Generator(self.device).manual_seed(seed)

        result = self.pipeline(
            prompt=full_prompt,
            negative_prompt=negative_prompt if negative_prompt else None,
            height=height,
            width=width,
            num_inference_steps=steps,
            guidance_scale=cfg,
            generator=generator,
            output_type="pil"
        )
        image = result.images[0]

        if lora_path:
            self.unload_lora()

        return image

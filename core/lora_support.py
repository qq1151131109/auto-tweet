"""
LoRAæ”¯æŒæ¨¡å—
æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
1. Diffusersæ¨¡å¼ï¼ˆæ¨èï¼‰ï¼šä½¿ç”¨diffusers.ZImagePipelineï¼ŒåŸç”Ÿæ”¯æŒLoRA
2. æ‰‹åŠ¨mergeæ¨¡å¼ï¼šåŠ è½½LoRAæƒé‡æ‰‹åŠ¨mergeåˆ°transformer
"""
from pathlib import Path
from typing import Optional, Dict
import torch
from safetensors.torch import load_file
import logging

logger = logging.getLogger(__name__)


def load_lora_diffusers(pipeline, lora_path: str, lora_strength: float = 1.0):
    """
    ä½¿ç”¨diffusersåŠ è½½LoRAï¼ˆæ¨èæ–¹æ¡ˆï¼‰

    Args:
        pipeline: ZImagePipelineå®ä¾‹
        lora_path: LoRAæ–‡ä»¶è·¯å¾„
        lora_strength: LoRAå¼ºåº¦
    """
    try:
        # Diffusersæ”¯æŒç›´æ¥åŠ è½½LoRA
        pipeline.load_lora_weights(lora_path)

        # è®¾ç½®LoRAå¼ºåº¦ï¼ˆå¦‚æœæ”¯æŒï¼‰
        if hasattr(pipeline, 'set_adapters'):
            pipeline.set_adapters(["default"], adapter_weights=[lora_strength])

        logger.info(f"âœ“ LoRAåŠ è½½æˆåŠŸï¼ˆdiffusersï¼‰: {lora_path}")
        return True

    except Exception as e:
        logger.error(f"âŒ LoRAåŠ è½½å¤±è´¥: {e}")
        return False


def merge_lora_to_transformer(
    transformer,
    lora_path: str,
    lora_strength: float = 1.0,
    device: str = "cuda"
) -> torch.nn.Module:
    """
    æ‰‹åŠ¨merge LoRAæƒé‡åˆ°transformerï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰

    Args:
        transformer: Transformeræ¨¡å‹
        lora_path: LoRA safetensorsæ–‡ä»¶è·¯å¾„
        lora_strength: LoRAå¼ºåº¦ï¼ˆalphaå€¼ï¼‰
        device: è®¾å¤‡

    Returns:
        åˆå¹¶åçš„transformer
    """
    lora_path = Path(lora_path)

    if not lora_path.exists():
        logger.warning(f"âš ï¸  LoRAæ–‡ä»¶ä¸å­˜åœ¨: {lora_path}")
        return transformer

    try:
        logger.info(f"ğŸ”§ æ‰‹åŠ¨merge LoRA: {lora_path.name}")
        logger.info(f"   å¼ºåº¦: {lora_strength}")

        # åŠ è½½LoRAæƒé‡
        lora_state_dict = load_file(str(lora_path), device=str(device))

        # LoRAæ ¼å¼é€šå¸¸æ˜¯: {layer_name}.lora_A.weight, {layer_name}.lora_B.weight
        # éœ€è¦æ‰¾åˆ°å¯¹åº”çš„transformerå±‚å¹¶merge

        transformer_state = transformer.state_dict()
        merged_count = 0

        # æå–LoRAå±‚å¯¹
        lora_pairs = {}
        for key in lora_state_dict.keys():
            if '.lora_A.' in key:
                base_name = key.replace('.lora_A.weight', '')
                if base_name not in lora_pairs:
                    lora_pairs[base_name] = {}
                lora_pairs[base_name]['A'] = lora_state_dict[key]
            elif '.lora_B.' in key:
                base_name = key.replace('.lora_B.weight', '')
                if base_name not in lora_pairs:
                    lora_pairs[base_name] = {}
                lora_pairs[base_name]['B'] = lora_state_dict[key]

        # Merge LoRAåˆ°åŸå§‹æƒé‡
        for base_name, lora_weights in lora_pairs.items():
            if 'A' not in lora_weights or 'B' not in lora_weights:
                continue

            # æ‰¾åˆ°å¯¹åº”çš„transformerå±‚
            target_key = base_name + '.weight'
            if target_key in transformer_state:
                # LoRAå…¬å¼: W' = W + alpha * (B @ A)
                lora_A = lora_weights['A'].to(device)
                lora_B = lora_weights['B'].to(device)

                delta_weight = lora_strength * (lora_B @ lora_A)
                transformer_state[target_key] += delta_weight

                merged_count += 1

        # åŠ è½½åˆå¹¶åçš„æƒé‡
        transformer.load_state_dict(transformer_state)

        logger.info(f"   âœ“ æˆåŠŸmerge {merged_count} ä¸ªLoRAå±‚")
        return transformer

    except Exception as e:
        logger.error(f"   âŒ LoRA mergeå¤±è´¥: {e}")
        return transformer


def get_lora_metadata(lora_path: str) -> Dict:
    """
    è¯»å–LoRAå…ƒæ•°æ®

    Args:
        lora_path: LoRAæ–‡ä»¶è·¯å¾„

    Returns:
        å…ƒæ•°æ®å­—å…¸
    """
    try:
        from safetensors import safe_open

        metadata = {}
        with safe_open(lora_path, framework="pt") as f:
            metadata = f.metadata() if hasattr(f, 'metadata') else {}

        return metadata

    except Exception as e:
        logger.warning(f"âš ï¸  æ— æ³•è¯»å–LoRAå…ƒæ•°æ®: {e}")
        return {}


class LoRAManager:
    """LoRAç®¡ç†å™¨ - ç»Ÿä¸€ç®¡ç†LoRAåŠ è½½å’Œå¸è½½"""

    def __init__(self, use_diffusers: bool = True):
        """
        Args:
            use_diffusers: æ˜¯å¦ä½¿ç”¨diffusersæ¨¡å¼ï¼ˆæ¨èTrueï¼‰
        """
        self.use_diffusers = use_diffusers
        self.loaded_loras = {}

    def load(
        self,
        model_or_pipeline,
        lora_path: str,
        lora_strength: float = 1.0,
        device: str = "cuda"
    ):
        """
        åŠ è½½LoRA

        Args:
            model_or_pipeline: Transformeræˆ–Pipeline
            lora_path: LoRAè·¯å¾„
            lora_strength: å¼ºåº¦
            device: è®¾å¤‡
        """
        if not lora_path or not Path(lora_path).exists():
            logger.warning(f"âš ï¸  è·³è¿‡LoRAåŠ è½½: {lora_path}")
            return model_or_pipeline

        lora_key = str(lora_path)

        # æ£€æŸ¥æ˜¯å¦å·²åŠ è½½
        if lora_key in self.loaded_loras:
            logger.info(f"âœ“ LoRAå·²åŠ è½½ï¼ˆç¼“å­˜ï¼‰: {Path(lora_path).name}")
            return model_or_pipeline

        # é€‰æ‹©åŠ è½½æ–¹å¼
        if self.use_diffusers and hasattr(model_or_pipeline, 'load_lora_weights'):
            # Diffusersæ¨¡å¼
            success = load_lora_diffusers(model_or_pipeline, lora_path, lora_strength)
            if success:
                self.loaded_loras[lora_key] = {
                    "strength": lora_strength,
                    "mode": "diffusers"
                }
        else:
            # æ‰‹åŠ¨mergeæ¨¡å¼
            model_or_pipeline = merge_lora_to_transformer(
                model_or_pipeline, lora_path, lora_strength, device
            )
            self.loaded_loras[lora_key] = {
                "strength": lora_strength,
                "mode": "manual"
            }

        return model_or_pipeline

    def unload_all(self, pipeline_or_model):
        """å¸è½½æ‰€æœ‰LoRA"""
        if hasattr(pipeline_or_model, 'unload_lora_weights'):
            pipeline_or_model.unload_lora_weights()
            logger.info("âœ“ å¸è½½æ‰€æœ‰LoRAï¼ˆdiffusersï¼‰")

        self.loaded_loras.clear()

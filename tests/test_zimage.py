#!/usr/bin/env python3
"""
æµ‹è¯• Z-Image ç¯å¢ƒé…ç½®
"""
import sys
import torch
from pathlib import Path
import logging

logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_environment():
    """æµ‹è¯•ç¯å¢ƒé…ç½®"""
    logger.info("=" * 60)
    logger.info("ğŸ§ª æµ‹è¯• Z-Image ç¯å¢ƒé…ç½®")
    logger.info("=" * 60)

    # 1. æ£€æŸ¥ Python ç‰ˆæœ¬
    logger.info(f"\n1ï¸âƒ£ Python ç‰ˆæœ¬: {sys.version}")

    # 2. æ£€æŸ¥ PyTorch
    logger.info(f"\n2ï¸âƒ£ PyTorch ç‰ˆæœ¬: {torch.__version__}")
    logger.info(f"   CUDA å¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        logger.info(f"   CUDA ç‰ˆæœ¬: {torch.version.cuda}")
        logger.info(f"   GPU æ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            logger.info(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
            memory_gb = torch.cuda.get_device_properties(i).total_memory / 1024**3
            logger.info(f"          æ˜¾å­˜: {memory_gb:.2f} GB")

    # 3. æ£€æŸ¥ diffusers
    try:
        import diffusers
        logger.info(f"\n3ï¸âƒ£ Diffusers ç‰ˆæœ¬: {diffusers.__version__}")

        # æ£€æŸ¥æ˜¯å¦æ”¯æŒ Z-Image
        try:
            from diffusers import ZImagePipeline
            logger.info("   âœ… ZImagePipeline å¯ç”¨")
        except ImportError:
            logger.error("   âŒ ZImagePipeline ä¸å¯ç”¨")
            return False
    except ImportError:
        logger.error("\n3ï¸âƒ£ Diffusers æœªå®‰è£…")
        return False

    # 4. æ£€æŸ¥å…¶ä»–ä¾èµ–
    logger.info("\n4ï¸âƒ£ å…¶ä»–ä¾èµ–:")
    try:
        import transformers
        logger.info(f"   Transformers: {transformers.__version__}")
    except:
        logger.error("   Transformers: âŒ æœªå®‰è£…")

    try:
        import safetensors
        logger.info(f"   Safetensors: âœ…")
    except:
        logger.error("   Safetensors: âŒ æœªå®‰è£…")

    try:
        from PIL import Image
        logger.info(f"   Pillow: âœ…")
    except:
        logger.error("   Pillow: âŒ æœªå®‰è£…")

    # 5. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    model_path = Path("Z-Image/ckpts/Z-Image-Turbo")
    logger.info(f"\n5ï¸âƒ£ æ¨¡å‹æ–‡ä»¶æ£€æŸ¥:")
    logger.info(f"   æ¨¡å‹è·¯å¾„: {model_path.absolute()}")
    logger.info(f"   è·¯å¾„å­˜åœ¨: {model_path.exists()}")

    if model_path.exists():
        # æ£€æŸ¥å…³é”®æ–‡ä»¶
        key_files = [
            "model_index.json",
            "scheduler_config.json",
            "text_encoder/config.json",
            "transformer/config.json",
            "vae/config.json"
        ]

        for file in key_files:
            file_path = model_path / file
            status = "âœ…" if file_path.exists() else "âŒ"
            logger.info(f"   {status} {file}")

        # æ£€æŸ¥æ¨¡å‹æƒé‡
        safetensors_files = list(model_path.glob("**/*.safetensors"))
        logger.info(f"\n   æ¨¡å‹æƒé‡æ–‡ä»¶æ•°é‡: {len(safetensors_files)}")
        total_size = sum(f.stat().st_size for f in safetensors_files)
        logger.info(f"   æ€»å¤§å°: {total_size / 1024**3:.2f} GB")

    logger.info("\n" + "=" * 60)
    logger.info("âœ… ç¯å¢ƒæ£€æŸ¥å®Œæˆ")
    logger.info("=" * 60)
    return True


def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ”„ æµ‹è¯•æ¨¡å‹åŠ è½½")
    logger.info("=" * 60)

    try:
        from diffusers import ZImagePipeline

        model_path = "Z-Image/ckpts/Z-Image-Turbo"
        logger.info(f"\næ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        logger.info("âš ï¸ è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")

        # åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨ float32 ä»¥å‡å°‘æ˜¾å­˜ä½¿ç”¨ï¼‰
        device = "cuda" if torch.cuda.is_available() else "cpu"
        dtype = torch.float32  # æµ‹è¯•æ—¶ä½¿ç”¨ float32

        logger.info(f"è®¾å¤‡: {device}")
        logger.info(f"æ•°æ®ç±»å‹: {dtype}")

        pipeline = ZImagePipeline.from_pretrained(
            model_path,
            torch_dtype=dtype,
            low_cpu_mem_usage=False
        )
        pipeline.to(device)

        logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")

        # æ£€æŸ¥æ¨¡å‹ç»„ä»¶
        logger.info("\næ¨¡å‹ç»„ä»¶:")
        logger.info(f"   Text Encoder: {type(pipeline.text_encoder).__name__}")
        logger.info(f"   Transformer: {type(pipeline.transformer).__name__}")
        logger.info(f"   VAE: {type(pipeline.vae).__name__}")
        logger.info(f"   Scheduler: {type(pipeline.scheduler).__name__}")

        # æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨ï¼ˆå¦‚æœä½¿ç”¨ CUDAï¼‰
        if device == "cuda":
            allocated = torch.cuda.memory_allocated(0) / 1024**3
            reserved = torch.cuda.memory_reserved(0) / 1024**3
            logger.info(f"\næ˜¾å­˜ä½¿ç”¨:")
            logger.info(f"   å·²åˆ†é…: {allocated:.2f} GB")
            logger.info(f"   å·²ä¿ç•™: {reserved:.2f} GB")

        logger.info("\n" + "=" * 60)
        logger.info("âœ… æ¨¡å‹åŠ è½½æµ‹è¯•å®Œæˆ")
        logger.info("=" * 60)
        return True

    except Exception as e:
        logger.error(f"\nâŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="æµ‹è¯• Z-Image ç¯å¢ƒé…ç½®")
    parser.add_argument(
        "--skip-model-loading",
        action="store_true",
        help="è·³è¿‡æ¨¡å‹åŠ è½½æµ‹è¯•ï¼ˆèŠ‚çœæ—¶é—´ï¼‰"
    )

    args = parser.parse_args()

    # æµ‹è¯•ç¯å¢ƒ
    env_ok = test_environment()

    if not env_ok:
        logger.error("\nâŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¾èµ–å®‰è£…")
        sys.exit(1)

    # æµ‹è¯•æ¨¡å‹åŠ è½½ï¼ˆå¯é€‰ï¼‰
    if not args.skip_model_loading:
        model_ok = test_model_loading()
        if not model_ok:
            logger.error("\nâŒ æ¨¡å‹åŠ è½½å¤±è´¥")
            sys.exit(1)
    else:
        logger.info("\nâ­ï¸ è·³è¿‡æ¨¡å‹åŠ è½½æµ‹è¯•")

    logger.info("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Z-Image ç¯å¢ƒé…ç½®æ­£ç¡®ã€‚")
